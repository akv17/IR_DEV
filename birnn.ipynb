{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras import layers as L\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "LE_CHANNEL = [\n",
    "    np.array([1, 2, 4, 5]),\n",
    "    np.array([1, 2, 4, 5])\n",
    "]\n",
    "RI_CHANNEL = [\n",
    "    np.array([3, 2, 4]),\n",
    "    np.array([1, 8, 9])\n",
    "]\n",
    "LABELS = np.array([1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_EMBED = np.random.normal(size=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10\n",
    "PRE_EMBED_SIZE = 8\n",
    "TR_EMBED_SIZE = 8\n",
    "BATCH_SIZE = 1\n",
    "WINDOW_SIZE = 2\n",
    "RNN_STATE_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object2vec_sentences():\n",
    "    # INPUT CHANNELS\n",
    "    # SHAPE_OUT: (batch, seq_len)\n",
    "    \n",
    "    le_channel = L.Input(\n",
    "        shape=(None,),\n",
    "        dtype='int32',\n",
    "        name='LEFT_CHANNEL'\n",
    "    )\n",
    "    \n",
    "    ri_channel = L.Input(\n",
    "        shape=(None,),\n",
    "        dtype='int32',\n",
    "        name='RIGHT_CHANNEL'\n",
    "    )\n",
    "    \n",
    "    # PRE_TRAINED EMBEDDINGS \n",
    "\n",
    "    pre_embeddings = L.Embedding(\n",
    "        input_dim=VOCAB_SIZE,\n",
    "        output_dim=PRE_EMBED_SIZE,\n",
    "        weights=[PRE_EMBED],\n",
    "        trainable=False,\n",
    "        name='PRE_EMBEDDINGS'\n",
    "    )\n",
    "    \n",
    "    # CHANNELS EMBEDDINGS\n",
    "    # SHAPE_OUT: (batch x seq_len x embed_size)\n",
    "    \n",
    "    le_embed = pre_embeddings(le_channel)\n",
    "    ri_embed = pre_embeddings(ri_channel)\n",
    "    \n",
    "    # CHANNELS ENCODERS\n",
    "    # SHAPE_OUT: (batch x rnn_state_size * 2)\n",
    "    \n",
    "    le_encoder = L.Bidirectional(\n",
    "        layer=L.LSTM(RNN_STATE_SIZE),\n",
    "        name='LEFT_ENCODER'\n",
    "    )(le_embed)\n",
    "    \n",
    "    ri_encoder = L.Bidirectional(\n",
    "        layer=L.LSTM(RNN_STATE_SIZE),\n",
    "        name='RIGHT_ENCODER'\n",
    "    )(ri_embed)\n",
    "    \n",
    "    # COMPARATOR:  HADAMARD PROD\n",
    "    # SHAPE_OUT: (batch x rnn_state_size * 2)\n",
    "    \n",
    "    comparator = L.Lambda(lambda x: tf.multiply(*x))([le_encoder, ri_encoder])\n",
    "    \n",
    "    # DISCRIMINATOR\n",
    "    discriminator = L.Dense(\n",
    "        units=2,\n",
    "        activation='softmax',\n",
    "        name='DISCRIMINATOR'\n",
    "    )(comparator)\n",
    "    \n",
    "    return Model(inputs=[le_channel, ri_channel], outputs=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = object2vec_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "LEFT_CHANNEL (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "RIGHT_CHANNEL (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "PRE_EMBEDDINGS (Embedding)      (None, None, 8)      80          LEFT_CHANNEL[0][0]               \n",
      "                                                                 RIGHT_CHANNEL[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "LEFT_ENCODER (Bidirectional)    (None, 32)           3200        PRE_EMBEDDINGS[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "RIGHT_ENCODER (Bidirectional)   (None, 32)           3200        PRE_EMBEDDINGS[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 32)           0           LEFT_ENCODER[0][0]               \n",
      "                                                                 RIGHT_ENCODER[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "DISCRIMINATOR (Dense)           (None, 2)            66          lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,546\n",
      "Trainable params: 6,466\n",
      "Non-trainable params: 80\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.6903 - acc: 0.7500\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6894 - acc: 0.7500\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6885 - acc: 0.7500\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6876 - acc: 0.7500\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6867 - acc: 0.7500\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6857 - acc: 0.7500\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6847 - acc: 0.7500\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6838 - acc: 0.7500\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6828 - acc: 0.7500\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6817 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef804f6dd8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[LE_CHANNEL, RI_CHANNEL], y=LABELS, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.predict(x=[LE_CHANNEL[:2], RI_CHANNEL[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object2vec_words():\n",
    "    input_context = L.Input(\n",
    "        shape=(WINDOW_SIZE,),\n",
    "        dtype='int32',\n",
    "        name='INPUT_CONTEXTS'\n",
    "    )\n",
    "\n",
    "    input_target = L.Input(\n",
    "        shape=(1,),\n",
    "        dtype='int32',\n",
    "        name='INPUT_LABELS'\n",
    "    )\n",
    "\n",
    "    # PRE_TRAINED EMBEDDINGS \n",
    "\n",
    "    pre_embed = L.Embedding(\n",
    "        input_dim=VOCAB_SIZE,\n",
    "        output_dim=PRE_EMBED_SIZE,\n",
    "        weights=[corpus.W2V_EMBED],\n",
    "        trainable=False,\n",
    "        name='PRE_EMBEDDINGS'\n",
    "    )(input_context)\n",
    "\n",
    "    # TRAINABLE EMBEDDINGS \n",
    "\n",
    "    tr_embed = L.Embedding(\n",
    "        input_dim=VOCAB_SIZE,\n",
    "        output_dim=TR_EMBED_SIZE,\n",
    "        name='TRAINABLE_EMBEDDINGS'\n",
    "    )(input_target)\n",
    "\n",
    "    # COLLAPSE TRAINABLE EMBEDDINGS TO SHAPE: [batch_size x TR_EMBED_SIZE]\n",
    "\n",
    "    tr_embed = L.Reshape(\n",
    "        target_shape=(TR_EMBED_SIZE,),\n",
    "        name='COLLAPSED_TRAINABLE_EMBEDDINGS'\n",
    "    )(tr_embed)\n",
    "\n",
    "    # CONCAT TRAINABLE AND PRE-TRAINED EMBEDDINGS WITHIN ENCODER \n",
    "\n",
    "    tr_embed_to_encoder = L.Lambda(\n",
    "        lambda x: K.repeat(x, WINDOW_SIZE),\n",
    "        name='TRAINABLE_EMBEDDINGS_LAMBDA'\n",
    "    )(tr_embed) \n",
    "\n",
    "    encoder_input = L.Concatenate(name='ENCODER_INPUT')([pre_embed, tr_embed_to_encoder])\n",
    "\n",
    "    # ENCODING CONTEXT\n",
    "\n",
    "    encoder = L.Bidirectional(\n",
    "        layer=L.CuDNNLSTM(RNN_STATE_SIZE),\n",
    "        name='ENCODER'\n",
    "    )(encoder_input)\n",
    "\n",
    "    # MERGING ENCODED CONTEXT WITH TARGET EMBEDDINGS\n",
    "    # SHAPE: [batch_size x RNN_STATE_SIZE * 2 + TR_EMBED_SIZE]\n",
    "\n",
    "    comparator = L.Concatenate(name='COMPARATOR')([encoder, tr_embed])\n",
    "\n",
    "    # DENSE\n",
    "\n",
    "    dense_1 = L.Dense(\n",
    "        units=128,\n",
    "        activation='relu',\n",
    "        name='DENSE_1'\n",
    "    )(comparator)\n",
    "\n",
    "    # OUTPUT PREDICTIONS\n",
    "\n",
    "    dense_out = L.Dense(\n",
    "        units=2,\n",
    "        activation='softmax',\n",
    "        name='DENSE_OUT'\n",
    "    )(dense_1)\n",
    "    \n",
    "    return Model(inputs=[input_context, input_target], outputs=dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_fine_tune():\n",
    "    input_context = L.Input(\n",
    "        shape=(WINDOW_SIZE,),\n",
    "        dtype='int32',\n",
    "        name='INPUT_CONTEXTS'\n",
    "    )\n",
    "\n",
    "    input_target = L.Input(\n",
    "        shape=(1,),\n",
    "        dtype='int32',\n",
    "        name='INPUT_LABELS'\n",
    "    )\n",
    "\n",
    "    # PRE_TRAINED EMBEDDINGS \n",
    "\n",
    "    embedding = L.Embedding(\n",
    "        input_dim=VOCAB_SIZE,\n",
    "        output_dim=EMBED_SIZE,\n",
    "        weights=[corpus.W2V_EMBED],\n",
    "        trainable=True,\n",
    "        name='PRE_EMBEDDINGS'\n",
    "    )\n",
    "\n",
    "    context_embed = embedding(input_context)\n",
    "\n",
    "    # TRAINABLE EMBEDDINGS \n",
    "\n",
    "    target_embed = embedding(input_target)\n",
    "\n",
    "    # COLLAPSE TRAINABLE EMBEDDINGS TO SHAPE: [batch_size x TR_EMBED_SIZE]\n",
    "\n",
    "    target_embed = L.Reshape(\n",
    "        target_shape=(EMBED_SIZE,),\n",
    "        name='COLLAPSED_TRAINABLE_EMBEDDINGS'\n",
    "    )(target_embed)\n",
    "\n",
    "    # ENCODING CONTEXT\n",
    "\n",
    "    encoder = L.Bidirectional(\n",
    "        layer=L.CuDNNLSTM(RNN_STATE_SIZE),\n",
    "        name='ENCODER'\n",
    "    )(context_embed)\n",
    "\n",
    "    # MERGING ENCODED CONTEXT WITH TARGET EMBEDDINGS\n",
    "    # SHAPE: [batch_size x RNN_STATE_SIZE * 2 + TR_EMBED_SIZE]\n",
    "\n",
    "    comparator = L.Dot(axes=-1, name='COMPARATOR')([encoder, target_embed])\n",
    "\n",
    "    # DENSE\n",
    "\n",
    "    # dense_1 = Dense(\n",
    "    #     units=128,\n",
    "    #     activation='relu',\n",
    "    #     name='DENSE_1'\n",
    "    # )(comparator)\n",
    "\n",
    "    # OUTPUT PREDICTIONS\n",
    "\n",
    "    dense_out = L.Dense(\n",
    "        units=2,\n",
    "        activation='softmax',\n",
    "        name='DENSE_OUT'\n",
    "    )(comparator)\n",
    "    \n",
    "    return Model(inputs=[input_context, input_target], outputs=dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_context, input_labels], outputs=dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2), Dimension(8)])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.3144 - acc: 0.2200\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.2804 - acc: 0.3700\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.2388 - acc: 0.4300\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.1880 - acc: 0.5100\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.1179 - acc: 0.5200\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.0243 - acc: 0.4700\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 9.8916 - acc: 0.4200\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 9.7167 - acc: 0.3400\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 9.4753 - acc: 0.3100\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 9.1582 - acc: 0.2600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f30cf116eb8>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[CONTEXTS_DATA[:100], LABELS_DATA[:100]], y=LABELS_DATA[:100], epochs=10, batch_size=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.predict([CONTEXTS_DATA[:5], LABELS_DATA[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.2178265e-05, 3.1872823e-05, 3.1878761e-05, ..., 3.2254604e-05,\n",
       "        3.2435961e-05, 3.2233638e-05],\n",
       "       [3.1995096e-05, 3.1848977e-05, 3.2101099e-05, ..., 3.2322718e-05,\n",
       "        3.2207776e-05, 3.2370219e-05],\n",
       "       [3.2093802e-05, 3.2129152e-05, 3.1940173e-05, ..., 3.2114731e-05,\n",
       "        3.2211134e-05, 3.1697680e-05],\n",
       "       [3.1949268e-05, 3.1947879e-05, 3.1997937e-05, ..., 3.2049498e-05,\n",
       "        3.2346343e-05, 3.2284624e-05],\n",
       "       [3.2119133e-05, 3.2424490e-05, 3.1950032e-05, ..., 3.1951193e-05,\n",
       "        3.2050219e-05, 3.1822066e-05]], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_left + window_right\n",
    "tf_vocab_size = 9\n",
    "tf_fitting_embed_size = 8\n",
    "tf_batch_size = 1\n",
    "tf_window_size = 2\n",
    "tf_rnn_state_size = 16\n",
    "tf_embedding_dense_size = 32\n",
    "tf_pre_embedding = tf.constant(pre_embedding, dtype=tf.float32)\n",
    "\n",
    "tf_input_context = tf.placeholder(dtype=tf.int32, shape=(tf_batch_size, tf_window_size))\n",
    "tf_input_labels = tf.placeholder(dtype=tf.int32, shape=(tf_batch_size,))\n",
    "\n",
    "tf_fitting_embedding = tf.Variable(tf.truncated_normal(shape=(tf_vocab_size, tf_fitting_embed_size), stddev=0.1))\n",
    "tf_W_out = tf.Variable(tf.truncated_normal(shape=(tf_rnn_state_size * 2 + tf_fitting_embed_size, tf_vocab_size), stddev=0.1))\n",
    "tf_b_out = tf.Variable(tf.zeros(shape=(tf_vocab_size,)))\n",
    "\n",
    "\n",
    "# EMBEDDING TENSOR [batch_size x window_size x pre_embed_size]\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    tf_context_pre_embed = tf.nn.embedding_lookup(tf_pre_embedding, tf_input_context)\n",
    "\n",
    "tf_rnn_cell_fw = tf.nn.rnn_cell.LSTMCell(16)\n",
    "tf_rnn_cell_bw = tf.nn.rnn_cell.LSTMCell(16)\n",
    "tf_rnn_state = tf_rnn_cell.zero_state(tf_batch_size, dtype=tf.float32)\n",
    "\n",
    "tf_rnn_outputs, tf_rnn_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    dtype=tf.float32,\n",
    "    cell_fw=tf_rnn_cell_fw,\n",
    "    cell_bw=tf_rnn_cell_bw,\n",
    "    inputs=tf_context_pre_embed\n",
    ")\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    tf_context_fitting_embed = tf.nn.embedding_lookup(tf_fitting_embedding, tf_input_labels)\n",
    "\n",
    "# CONCAT FW AND BW HIDDEN STATES AND FITTING EMBED OF TARGETS [batch_size x HIDDEN_STATE_SIZE * 2 + FITTING_EMBED_SIZE]\n",
    "\n",
    "tf_final_state = tf.concat(\n",
    "    [tf_rnn_states[0].c, tf_rnn_states[1].c, tf_context_fitting_embed],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "tf_logits = tf.matmul(tf_final_state, tf_W_out) + tf_b_out\n",
    "\n",
    "tf_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=tf_input_labels,\n",
    "    logits=tf_logits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_init_op = tf.global_variables_initializer()\n",
    "tf_sess = tf.Session()\n",
    "tf_sess.run(tf_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tf_sess.run(\n",
    "        tf_final_state,\n",
    "        {tf_input_context: np.array([[0, 2]]), tf_input_labels: np.array([1])}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05075238, -0.2744412 , -0.23570031, -0.01703531, -0.00418064,\n",
       "         0.34944135,  0.30708185, -0.15840204,  0.632811  , -0.20224315,\n",
       "         0.20672935, -0.3125187 ,  0.16155767,  0.38695666, -0.1943923 ,\n",
       "         0.20426655]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat(\n",
    "    [tf.constant(np.array([[1]])), tf.constant(np.array([[2]]))], 1\n",
    ").eval(session=tf_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
