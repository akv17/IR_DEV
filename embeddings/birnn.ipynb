{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, LSTM, Bidirectional, Concatenate, Reshape, Lambda\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[\\n\\t\\r]', ' ', text)\n",
    "    text = re.sub(' +', 'SPACEPLACEHOLDER', text)\n",
    "    text = re.sub('[\\W]', '', text)\n",
    "    text = re.sub('SPACEPLACEHOLDER', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [text_normalize(text) for text in data[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    'раз два три четыре пять шесть семь',\n",
    "    'раз два пять восемь'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['NULL'] = 0\n",
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = dict()\n",
    "\n",
    "for w, idx in tokenizer.word_index.items():\n",
    "    if embeddings.get(idx) is None:\n",
    "        embeddings[idx] = np.random.normal(size=(8,)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_embedding = np.array([embeddings[i] for i in range(len(embeddings))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_generator(text, le, ri):\n",
    "    \"\"\"\n",
    "    text: <np.array> [n_words x embed_size]\n",
    "    le: <int>: left window\n",
    "    ri: <int>: right window\n",
    "    \"\"\"\n",
    "    \n",
    "    # `NULL` SIDE-PADDING\n",
    "    # word_index['NULL'] = 0\n",
    "        \n",
    "    text = [0] * le + text + [0] * ri \n",
    "    \n",
    "    # STREAMING BATCHES\n",
    "    # AS (center_word_idx, context_word_idx) PAIRS \n",
    "    \n",
    "    for i in range(le, len(text)-ri):\n",
    "        yield (tuple(text[i-le:i] + text[i+1:i+1+ri]), text[i])\n",
    "        \n",
    "\n",
    "def sample_negative(true_context, hi):\n",
    "    sampled_context = list()\n",
    "    \n",
    "    while len(sampled_context) < len(true_context):\n",
    "        sample = np.random.randint(0, hi)\n",
    "\n",
    "        while sample in true_context:\n",
    "            sample = np.random.randint(0, hi)\n",
    "        \n",
    "        sampled_context.append(sample)\n",
    "        \n",
    "    return sampled_context\n",
    "        \n",
    "def build_data(data,\n",
    "               tokenizer,\n",
    "               window_size=(1, 1),\n",
    "               neg_sampling_prob=1,\n",
    "               n_neg_samples=1\n",
    "              ):\n",
    "    \"\"\"\n",
    "    return: contexts: : [n_pairs x sum(window_size)]\n",
    "            targets: np.array: [n_pairs,]\n",
    "    \"\"\"\n",
    "    \n",
    "    contexts = list()\n",
    "    targets = list()\n",
    "    labels = list()\n",
    "    \n",
    "    for text in tokenizer.texts_to_sequences_generator(data):\n",
    "        for context, target in window_generator(text, *window_size):\n",
    "            contexts.append(context)\n",
    "            targets.append(target)\n",
    "            labels.append(1)\n",
    "            \n",
    "            if np.random.random() < neg_sampling_prob:\n",
    "                contexts.append(\n",
    "                    sample_negative(\n",
    "                        context,\n",
    "                        len(tokenizer.word_index)\n",
    "                    )\n",
    "                )\n",
    "                targets.append(target)\n",
    "                labels.append(0)\n",
    "            \n",
    "    contexts = np.array(contexts, dtype=np.int32)\n",
    "    targets = np.array(targets, dtype=np.int32)\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "      \n",
    "    return contexts, targets, labels\n",
    "    \n",
    "        \n",
    "def build_data_generator(data, tokenizer, embeddings, window_size=(1, 1)):\n",
    "    for text in tokenizer.texts_to_sequences_generator(data):\n",
    "        yield from window_generator(text, *window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 2]), list([3])], dtype=object)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 4, 5, 3, 6, 7], [1, 2, 3, 8]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTS_DATA, TARGETS_DATA, LABELS_DATA = build_data(data[:2], tokenizer, window_size=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0, 8809],\n",
       "        [3615, 9688]], dtype=int32),\n",
       " array([13, 13], dtype=int32),\n",
       " array([1, 0], dtype=int32))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXTS_DATA[:2], TARGETS_DATA[:2], LABELS_DATA[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in batch_generator(data, tokenizer, embeddings, window_size=(1, 1)): print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index)\n",
    "PRE_EMBED_SIZE = 8\n",
    "TR_EMBED_SIZE = 8\n",
    "BATCH_SIZE = 1\n",
    "WINDOW_SIZE = 2\n",
    "RNN_STATE_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_context = Input(\n",
    "    shape=(WINDOW_SIZE,),\n",
    "    dtype='int32',\n",
    "    name='INPUT_CONTEXTS'\n",
    ")\n",
    "\n",
    "input_labels = Input(\n",
    "    shape=(1,),\n",
    "    dtype='int32',\n",
    "    name='INPUT_LABELS'\n",
    ")\n",
    "\n",
    "# PRE_TRAINED EMBEDDINGS \n",
    "\n",
    "pre_embed = Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=PRE_EMBED_SIZE,\n",
    "    weights=[pre_embedding],\n",
    "    trainable=False,\n",
    "    name='PRE_EMBEDDINGS'\n",
    ")(input_context)\n",
    "\n",
    "# TRAINABLE EMBEDDINGS \n",
    "\n",
    "tr_embed = Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=TR_EMBED_SIZE,\n",
    "    name='TRAINABLE_EMBEDDINGS'\n",
    ")(input_labels)\n",
    "\n",
    "# COLLAPSE TRAINABLE EMBEDDINGS TO SHAPE: [batch_size x TR_EMBED_SIZE]\n",
    "\n",
    "tr_embed = Reshape(\n",
    "    target_shape=(TR_EMBED_SIZE,),\n",
    "    name='COLLAPSED_TRAINABLE_EMBEDDINGS'\n",
    ")(tr_embed)\n",
    "\n",
    "# CONCAT TRAINABLE AND PRE-TRAINED EMBEDDINGS WITHIN ENCODER \n",
    "\n",
    "tr_embed_to_encoder = Lambda(\n",
    "    lambda x: K.repeat(x, WINDOW_SIZE),\n",
    "    name='TRAINABLE_EMBEDDINGS_LAMBDA'\n",
    ")(tr_embed) \n",
    "\n",
    "encoder_input = Concatenate(name='ENCODER_INPUT')([pre_embed, tr_embed_to_encoder])\n",
    "\n",
    "# ENCODING CONTEXT\n",
    "\n",
    "encoder = Bidirectional(\n",
    "    layer=LSTM(RNN_STATE_SIZE),\n",
    "    name='ENCODER'\n",
    ")(encoder_input)\n",
    "\n",
    "# MERGING ENCODED CONTEXT WITH TARGET EMBEDDINGS\n",
    "# SHAPE: [batch_size x RNN_STATE_SIZE * 2 + TR_EMBED_SIZE]\n",
    "\n",
    "comparator = Concatenate(name='COMPARATOR')([encoder, tr_embed])\n",
    "\n",
    "# OUTPUT PREDICTIONS\n",
    "\n",
    "dense = Dense(\n",
    "    units=VOCAB_SIZE,\n",
    "    activation='softmax',\n",
    "    name='DENSE_OUT'\n",
    ")(comparator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_context, input_labels], outputs=dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2), Dimension(8)])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.3144 - acc: 0.2200\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.2804 - acc: 0.3700\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.2388 - acc: 0.4300\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.1880 - acc: 0.5100\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.1179 - acc: 0.5200\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 10.0243 - acc: 0.4700\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 9.8916 - acc: 0.4200\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 9.7167 - acc: 0.3400\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 9.4753 - acc: 0.3100\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 9.1582 - acc: 0.2600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f30cf116eb8>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[CONTEXTS_DATA[:100], LABELS_DATA[:100]], y=LABELS_DATA[:100], epochs=10, batch_size=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.predict([CONTEXTS_DATA[:5], LABELS_DATA[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.2178265e-05, 3.1872823e-05, 3.1878761e-05, ..., 3.2254604e-05,\n",
       "        3.2435961e-05, 3.2233638e-05],\n",
       "       [3.1995096e-05, 3.1848977e-05, 3.2101099e-05, ..., 3.2322718e-05,\n",
       "        3.2207776e-05, 3.2370219e-05],\n",
       "       [3.2093802e-05, 3.2129152e-05, 3.1940173e-05, ..., 3.2114731e-05,\n",
       "        3.2211134e-05, 3.1697680e-05],\n",
       "       [3.1949268e-05, 3.1947879e-05, 3.1997937e-05, ..., 3.2049498e-05,\n",
       "        3.2346343e-05, 3.2284624e-05],\n",
       "       [3.2119133e-05, 3.2424490e-05, 3.1950032e-05, ..., 3.1951193e-05,\n",
       "        3.2050219e-05, 3.1822066e-05]], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "INPUT_LABELS (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "TRAINABLE_EMBEDDINGS (Embedding (None, 1, 8)         249600      INPUT_LABELS[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "INPUT_CONTEXTS (InputLayer)     (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "COLLAPSED_TRAINABLE_EMBEDDINGS  (None, 8)            0           TRAINABLE_EMBEDDINGS[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "PRE_EMBEDDINGS (Embedding)      (None, 2, 8)         249600      INPUT_CONTEXTS[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TRAINABLE_EMBEDDINGS_LAMBDA (La (None, 2, 8)         0           COLLAPSED_TRAINABLE_EMBEDDINGS[0]\n",
      "__________________________________________________________________________________________________\n",
      "ENCODER_EMBEDDINGS (Concatenate (None, 2, 16)        0           PRE_EMBEDDINGS[0][0]             \n",
      "                                                                 TRAINABLE_EMBEDDINGS_LAMBDA[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "ENCODER (Bidirectional)         (None, 32)           4224        ENCODER_EMBEDDINGS[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "COMPARATOR (Concatenate)        (None, 40)           0           ENCODER[0][0]                    \n",
      "                                                                 COLLAPSED_TRAINABLE_EMBEDDINGS[0]\n",
      "__________________________________________________________________________________________________\n",
      "DENSE_OUT (Dense)               (None, 31200)        1279200     COMPARATOR[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,782,624\n",
      "Trainable params: 1,533,024\n",
      "Non-trainable params: 249,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_left + window_right\n",
    "tf_vocab_size = 9\n",
    "tf_fitting_embed_size = 8\n",
    "tf_batch_size = 1\n",
    "tf_window_size = 2\n",
    "tf_rnn_state_size = 16\n",
    "tf_embedding_dense_size = 32\n",
    "tf_pre_embedding = tf.constant(pre_embedding, dtype=tf.float32)\n",
    "\n",
    "tf_input_context = tf.placeholder(dtype=tf.int32, shape=(tf_batch_size, tf_window_size))\n",
    "tf_input_labels = tf.placeholder(dtype=tf.int32, shape=(tf_batch_size,))\n",
    "\n",
    "tf_fitting_embedding = tf.Variable(tf.truncated_normal(shape=(tf_vocab_size, tf_fitting_embed_size), stddev=0.1))\n",
    "tf_W_out = tf.Variable(tf.truncated_normal(shape=(tf_rnn_state_size * 2 + tf_fitting_embed_size, tf_vocab_size), stddev=0.1))\n",
    "tf_b_out = tf.Variable(tf.zeros(shape=(tf_vocab_size,)))\n",
    "\n",
    "\n",
    "# EMBEDDING TENSOR [batch_size x window_size x pre_embed_size]\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    tf_context_pre_embed = tf.nn.embedding_lookup(tf_pre_embedding, tf_input_context)\n",
    "\n",
    "tf_rnn_cell_fw = tf.nn.rnn_cell.LSTMCell(16)\n",
    "tf_rnn_cell_bw = tf.nn.rnn_cell.LSTMCell(16)\n",
    "tf_rnn_state = tf_rnn_cell.zero_state(tf_batch_size, dtype=tf.float32)\n",
    "\n",
    "tf_rnn_outputs, tf_rnn_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    dtype=tf.float32,\n",
    "    cell_fw=tf_rnn_cell_fw,\n",
    "    cell_bw=tf_rnn_cell_bw,\n",
    "    inputs=tf_context_pre_embed\n",
    ")\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    tf_context_fitting_embed = tf.nn.embedding_lookup(tf_fitting_embedding, tf_input_labels)\n",
    "\n",
    "# CONCAT FW AND BW HIDDEN STATES AND FITTING EMBED OF TARGETS [batch_size x HIDDEN_STATE_SIZE * 2 + FITTING_EMBED_SIZE]\n",
    "\n",
    "tf_final_state = tf.concat(\n",
    "    [tf_rnn_states[0].c, tf_rnn_states[1].c, tf_context_fitting_embed],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "tf_logits = tf.matmul(tf_final_state, tf_W_out) + tf_b_out\n",
    "\n",
    "tf_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=tf_input_labels,\n",
    "    logits=tf_logits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_init_op = tf.global_variables_initializer()\n",
    "tf_sess = tf.Session()\n",
    "tf_sess.run(tf_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tf_sess.run(\n",
    "        tf_final_state,\n",
    "        {tf_input_context: np.array([[0, 2]]), tf_input_labels: np.array([1])}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05075238, -0.2744412 , -0.23570031, -0.01703531, -0.00418064,\n",
       "         0.34944135,  0.30708185, -0.15840204,  0.632811  , -0.20224315,\n",
       "         0.20672935, -0.3125187 ,  0.16155767,  0.38695666, -0.1943923 ,\n",
       "         0.20426655]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat(\n",
    "    [tf.constant(np.array([[1]])), tf.constant(np.array([[2]]))], 1\n",
    ").eval(session=tf_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
